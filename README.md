### Отчёт:
Разработала парсер на Scrapy для сбора данных с сайта cian.py, https://kazan.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&p=1&region=4777&room1=1. Паук собирает данные из каждого объявления: заголовок объявления, адрес, цена, ссылка на обьявление, номер страницы. Работа хранится в файле cian.py. Собранные данные сохраняет в формате JSON. Обработка кнопки "Показать еще" находится в 8 строке, в перменной start_urls, здесь указывается количество страниц, с которых паук собирает данные. Обработала возможные исключения, добавив условия if len(rows) > 0: и if len(info_adress) > 0:. Мои предложения об улучшении парсера: разобраться с обработкой кнопкой "Показать еще", чтобы она считывала все страницы, а не указанное количество, реализовать использование прокси, реализовать возможность сбора дополнительных деталей об объявлениях.

Также частично разработала парсер на библиотеке bs4 (файл CianBeautifulSoup.py). Этот парсер собирает информацию о заголовке объявления, его адрес, цену и номер страницы в формате JSON.
## Технологии:
**Python 3.9.0**

**Scrapy**

**JSON**

**bs4**
## Как запустить:
Клонировать репозиторий и перейти в него в командной строке:
```
git clone git@github.com:ekaterinachigina/test_Spider.git
```
Cоздать и активировать виртуальное окружение:
```
python -m venv venv
```
```
source env/bin/activate
```
Установить фрeймворк Scrapy:
```
pip install scrapy
```
Установить библиотеку bs4:
```
pip install beautifulsoup4
```
Вывод результата в JSON:
```
scrapy crawl cian_spider --output result.json
```
